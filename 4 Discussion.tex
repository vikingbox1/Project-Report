\chapter{Discussion} \label{Chap5}
The search algorithm we developed performs well for smaller codes, as demonstrated in the results section. However, its success rate, both for individual climbers and the overall system with multiple parallel populations, declines significantly as the code size increases. This drop in performance is likely due to the exponential growth of the search space with increasing code dimensions.

One straightforward solution to address the low success rate for larger codes is to increase the number of parallel populations. In our experiments, we were limited to a maximum of eight populations per run, constrained by the eight-core Apple M1 processor used for testing. This limitation, however, can be overcome by deploying the algorithm on a high-performance computing (HPC) cluster, such as UCL's Legion cluster, which offers access to over 7,500 CPU cores. Utilising such HPC resources would not only allow for a larger number of concurrent populations, thereby improving the algorithm's exploratory capacity, but would also significantly accelerate testing and parameter tuning.

In our case, testing was primarily conducted on the \([[5,1,3]]\) and \([[7,1,3]]\) quantum error-correcting codes, as they allowed for relatively faster data collection, although even these cases were time-consuming. Due to the limited number of code sizes tested and the resulting scarcity of performance data, it is difficult to conclusively assess the algorithm's robustness or scalability to much larger codes.

Beyond computational limitations, the algorithm itself still has room for improvement. One area worth exploring is the initial generation of the matrices \(U_C\) and \(U_A\). Currently, these matrices are initialised completely at random, which may lead some populations beginning their search in unfavorable regions of the search space, potentially getting stuck in local minima early on. Moreover, the lack of communication or coordination between initial populations means we cannot guarantee sufficient diversity at the start of each run. This can result in multiple populations redundantly exploring the same regions, thereby reducing overall efficiency.

Another promising direction for future enhancement is the integration of reinforcement learning (RL). RL strategies could be employed to dynamically control aspects of the search process, such as the type and location of mutations. Incorporating learning-based guidance could help the algorithm adapt more intelligently to the structure of the search space over time, improving both convergence speed and solution quality.

As demonstrated in the results section, we were able to find a constant-depth implementation of the logical \(S\) gate, along with relatively low-depth decompositions of the logical \(H\), \(CZ\), and \(CNOT\) operators. To the best of our knowledge, both the method used to discover these logical operators and the specific implementations themselves have not been previously documented in the literature. One notable related work is the recent paper Fault-Tolerant Constant-Depth Clifford Gates on Toric Codes by Alexandre Guernut and Christophe Vuillot \cite{guernut2024faulttolerantconstantdepthcliffordgates}, where the authors propose a framework for implementing the full Clifford group on 2D toric codes using fault-tolerant techniques such as fold-transversal gates, Dehn twists, and single-shot logical Pauli measurements.

While their approach is tailored specifically to topological codes like the toric code and relies on code-specific symmetries and geometric operations, the method presented in this paper is fundamentally different. Our algorithmic framework offers a more general and adaptable strategy that is not restricted to topological codes and can be applied to a wide variety of stabiliser codes. This broader applicability makes our approach particularly useful for synthesising logical Clifford gates in more diverse quantum error correction architectures.
